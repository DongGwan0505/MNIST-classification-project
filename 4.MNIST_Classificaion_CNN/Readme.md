ğŸ§  ITì‹œìŠ¤í…œì¢…í•©ì„¤ê³„ í”„ë¡œì íŠ¸ â€“ CNN ê¸°ë°˜ MNIST ì´ë¯¸ì§€ ë¶„ë¥˜
ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” 2024-1 ITì‹œìŠ¤í…œì¢…í•©ì„¤ê³„ ê³¼ëª©ì—ì„œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
ê¸°ì¡´ì˜ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (FCNN) ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ì„ í™•ì¥í•˜ì—¬, í•©ì„±ê³± ì‹ ê²½ë§(CNN)ì„ í™œìš©í•´ MNIST ìˆ«ì ì´ë¯¸ì§€ ë¶„ë¥˜ ì •í™•ë„ë¥¼ ê°œì„ í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆìŠµë‹ˆë‹¤.

ğŸš€ ì—°êµ¬ ëª©í‘œ

Baseline ì„±ëŠ¥ í™•ë³´: FCNNìœ¼ë¡œ ì•½ 99.3% ì •í™•ë„ ë‹¬ì„±

CNN ì ìš©: Conv2D + MaxPoolingì„ ì ìš©í•´ ì´ë¯¸ì§€ì˜ ê³µê°„ì  íŠ¹ì§• í•™ìŠµ

ì„±ëŠ¥ ê°œì„  ê¸°ë²• ì ìš©

Batch Normalization & He Initialization

Dropoutìœ¼ë¡œ ê³¼ì í•© ë°©ì§€

Data Augmentation (rotation, shift, zoom)

Optimizer: Nadam + ReduceLROnPlateau

Global Average Pooling(GAP) ì¶”ê°€

ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°
model = keras.models.Sequential([
    Conv2D(64, kernel_size=7, padding="same", activation="relu", input_shape=[28, 28, 1]),
    BatchNormalization(),
    MaxPooling2D(pool_size=2),

    Conv2D(128, kernel_size=3, padding="same", activation="relu"),
    BatchNormalization(),
    Conv2D(128, kernel_size=3, padding="same", activation="relu"),
    BatchNormalization(),
    MaxPooling2D(pool_size=2),

    Conv2D(256, kernel_size=3, padding="same", activation="relu"),
    BatchNormalization(),
    Conv2D(256, kernel_size=3, padding="same", activation="relu"),
    BatchNormalization(),

    GlobalAveragePooling2D(),

    Dense(128, activation="relu"), Dropout(0.5),
    Dense(64, activation="relu"), Dropout(0.5),
    Dense(10, activation="softmax")
])

ğŸ“Š ì‹¤í—˜ ê²°ê³¼
ëª¨ë¸	íŠ¹ì§•	ì •í™•ë„
FCNN (Baseline)	BatchNorm, Dropout, Adam, ì¦ê°• ì ìš©	99.3%
CNN (ê¸°ë³¸)	Conv2D + MaxPooling	98.8%
CNN (ì •ê·œí™” + He Init)	BatchNorm ì¶”ê°€	99.4%
CNN (GAP ì¶”ê°€ + êµ¬ì¡° ìµœì í™”)	Global Avg Pooling	99.5%
CNN (ìµœì¢…)	Nadam + ReduceLROnPlateau ì ìš©	99.7%

â¡ï¸ ìµœì¢…ì ìœ¼ë¡œ CNN ëª¨ë¸ì´ ìµœê³  ì„±ëŠ¥ (99.7%) ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

ğŸ¯ ë°°ìš´ ì 

ì´ë¯¸ì§€ ë°ì´í„°ëŠ” FCNNë³´ë‹¤ CNNì´ ì í•©í•˜ë©°, ê³µê°„ì  íŒ¨í„´ í•™ìŠµì— ìœ ë¦¬í•˜ë‹¤.

Batch Normalization, He Initialization, Dropout, Data Augmentation ì´ ì„±ëŠ¥ í–¥ìƒì— í¬ê²Œ ê¸°ì—¬í–ˆë‹¤.

Learning Rate Scheduling (ReduceLROnPlateau)ê³¼ Optimizer ì„ íƒ(Nadam)ì´ ìµœì¢… ì„±ëŠ¥ ê°œì„ ì˜ í•µì‹¬ì´ì—ˆë‹¤.

âš™ï¸ ì‹¤í–‰ ë°©ë²•
1. ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/yourusername/mnist-cnn-itdesign.git
cd mnist-cnn-itdesign

2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install -r requirements.txt

3. í•™ìŠµ ì‹¤í–‰
python train_cnn.py

4. Colabì—ì„œ ì‹¤í–‰

Google Colab í™˜ê²½ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:

!git clone https://github.com/yourusername/mnist-cnn-itdesign.git
%cd mnist-cnn-itdesign
!pip install -r requirements.txt
!python train_cnn.py

ğŸ“š ì°¸ê³ 

MNIST Dataset (Yann LeCun et al.)

TensorFlow / Keras ê³µì‹ ë¬¸ì„œ
